{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALL 20 folder names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groups_for_train = ['alt.atheism', 'comp.graphics' , 'comp.os.ms-windows.misc' , 'comp.sys.ibm.pc.hardware' , 'comp.sys.mac.hardware' , 'comp.windows.x' , 'misc.forsale' , 'rec.autos' , 'rec.motorcycles' , 'rec.sport.baseball' , 'rec.sport.hockey' , 'sci.crypt' , 'sci.electronics' , 'sci.med' , 'sci.space' , 'soc.religion.christian' , 'talk.politics.guns' , 'talk.politics.mideast' , 'talk.politics.misc' ,'talk.religion.misc' ]\n",
    "len(groups_for_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for Stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  Remove_stop_words(x):\n",
    "    l=len(x)\n",
    "    \n",
    "    y=[]\n",
    "    stop_words=['a','for','Not','not','NOT','are','when','were','then','them','some','also','just','1993','as','on','if','will','here','there','their','with','will','all','what','at','my','who','but','so','or','would','can','be','i','you','by','your','we','was','an','that','they','which','the','in','are,''that','he','she','should','you','yours','it','has','have','is','this','had','could','from','to','of','and','an','A','For','Are','When','Were','Then','Them','Some','Also','Just','1993','As','On','If','Will','Here','There','Their','With','Will','All','What','At','My','Who','But','So','Or','Would','Can','Be','I','You','By','Your','We','Was','An','That','They','Which','The','In','Are,''That','He','She','Should','You','Yours','It','Has','Have','Is','This','Had','Could','From','To','of','And','An']\n",
    "    \n",
    "    for i in range(l):\n",
    "        if x[i] not in stop_words :\n",
    "            y.append(x[i])\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for lower case all extracted words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_casing(words): \n",
    "    text=[]\n",
    "    for i in words :\n",
    "        text.append(i.lower())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for Lemmatization (using nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(words):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    x=[]\n",
    "    for i in words:\n",
    "        x.append(lemmatizer.lemmatize(i))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(words):\n",
    "    for i in range(len(words)):\n",
    "            if not words[i].isalpha() :       # making all the words as empty string which are not alphabets\n",
    "                words[i] = ''\n",
    "    new_words=[]\n",
    "    for i in words :   \n",
    "        if (i != ''):\n",
    "            new_words.append(i)               # now getting only alphabets from the words list \n",
    "\n",
    "    new_words = lower_casing(new_words)       # lower casing all the words , ex: (Good --> good)\n",
    "    \n",
    "    return new_words\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for getting words greater than length of 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def size_changer(words):\n",
    "    x=[]\n",
    "    for i in words:\n",
    "        if (len(i) > 3):       # taking only those words whose length is greater than 3\n",
    "            x.append(i)\n",
    "    return x\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for extracting words only for vocabulary(only from train data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getting_data_for_vocab(x):\n",
    "    direct = r'20 news group dataset\\Train data\\\\' + x + '/'\n",
    "    files = os.listdir(direct)\n",
    "\n",
    "    alt = [direct + i for i in files]           # extracting all files from a particular folders x( from trainging data)\n",
    "    words=[]\n",
    "    \n",
    "    for j in alt :\n",
    "        f = open(j)\n",
    "        reading = f.read()                      # extracting all text from all files\n",
    "        words +=reading.split(' ')\n",
    "     \n",
    "    \n",
    "    new_words = tokenizer(words)                # applying tokenization \n",
    "    \n",
    "    new_words = lemmatization(new_words)        # applying lemmatization\n",
    "   \n",
    "    return new_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for extracting top most common words from words list (using Counter Function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_dictionary(x):\n",
    "    words=x\n",
    "    words_without_stop_words = Remove_stop_words(words)                        # removing stop words\n",
    "    \n",
    "    words_with_length_greater_than_3 = size_changer(words_without_stop_words)  # take out all words greater than 3\n",
    "    \n",
    "    dictionary = Counter(words_with_length_greater_than_3)                     # using (counter function)\n",
    "    \n",
    "    x = dictionary.most_common(5000)                                        \n",
    "    \n",
    "    return x\n",
    "  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function call for vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vocab=[]\n",
    "for i in groups_for_train:\n",
    "    vocab += getting_data_for_vocab(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fucntion call for Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "My_dictionary=make_dictionary(vocab)\n",
    "type(My_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(My_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function  for making documnet for Train Data (with applying all functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getting_docs_for_train(x,y):\n",
    "    direct = r'20 news group dataset\\Train data\\\\' + x + '/'\n",
    "    \n",
    "    files = os.listdir(direct)                           # extracting all files from a particular folder x(from train data)\n",
    "\n",
    "    alt = [direct + i for i in files]         \n",
    "    words=[]\n",
    "    \n",
    "    data = []                                       \n",
    "    y_values = []\n",
    "    for j in alt :                                      # extracting all text from all files and make a seperate row for every files \n",
    "        f = open(j)                                        \n",
    "        reading = f.read()                                 \n",
    "        words =reading.split(' ')                         \n",
    "        \n",
    "        new_words = tokenizer(words)                    # applying tokenizer on the text of particular file\n",
    "        \n",
    "        new_words = lemmatization(new_words)            # applying lemmatization on the text of particular file\n",
    "        \n",
    "        new_words = Remove_stop_words(new_words)        # removing stop words from text of particular file\n",
    "        \n",
    "        new_words = size_changer(new_words)             # taking all words with lenth greater than 3\n",
    "        \n",
    "        data.append(new_words)                          # appending the list of words of a particular file as a row in data(it becomes a 2d list)\n",
    "        y_values.append(y)                              # appending the number of folder which the data belongs \n",
    "\n",
    "    return data,y_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop which calls function(getting_docs_for_train) for every folder(groups_for_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16315\n",
      "16315\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "y=0\n",
    "Y_train=[]\n",
    "for i in groups_for_train :                       # loops for every folder\n",
    "    datas=0\n",
    "    y_val=0\n",
    "    datas,y_val = getting_docs_for_train(i,y)     # function call on every folder (which is 20)\n",
    "    data += datas\n",
    "    Y_train += y_val                             \n",
    "    y += 1\n",
    "print(len(data))\n",
    "print(len(Y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop for creating actual dataset for x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(data))\n",
    "print(type(My_dictionary))\n",
    "X_train= np.zeros((len(data),len(My_dictionary)))    # creating a dummy datasets with all zeros\n",
    "\n",
    "for row in range(len(data)):                         \n",
    "    for word in data[row]:\n",
    "        for column in range(len(My_dictionary)):\n",
    "            if(word==My_dictionary[column][0]):      # if that words is matching with the current feature words\n",
    "                X_train[row][column]+=1              # then we will increment that place by one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function  for making documnet for Test Data (with applying all functions)\n",
    "\n",
    "### Note: all functions are same as i did for train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getting_docs_for_test(x,y):\n",
    "    direct = r'20 news group dataset\\Test data\\\\' + x + '/'\n",
    "    files = os.listdir(direct)\n",
    "\n",
    "    alt = [direct + i for i in files]\n",
    "    words=[]\n",
    "\n",
    "    data = []\n",
    "    y_values = []\n",
    "    for j in alt :\n",
    "        f = open(j)\n",
    "        reading = f.read()\n",
    "        words =reading.split(' ')\n",
    "        new_words = tokenizer(words)\n",
    "        new_words = lemmatization(new_words)\n",
    "        new_words = Remove_stop_words(new_words)\n",
    "        new_words = size_changer(new_words)\n",
    "        data.append(new_words)\n",
    "        y_values.append(y)\n",
    "\n",
    "    return data,y_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups_for_test=['0','1','2','3','4','5','6','7','8','9','10','11','12','13','14','15','16','17','18','19']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop which calls function(getting_docs_for_test) for every folder(groups_for_test)\n",
    "\n",
    "### Note: all is same ass i did for train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3682\n",
      "3682\n"
     ]
    }
   ],
   "source": [
    "data_for_test = []\n",
    "y=0\n",
    "Y_test=[]\n",
    "for i in groups_for_test :\n",
    "    datas=0\n",
    "    y_val=0\n",
    "    datas,y_val = getting_docs_for_test(i,y)\n",
    "    data_for_test += datas\n",
    "    Y_test += y_val\n",
    "    y += 1\n",
    "print(len(data_for_test))\n",
    "print(len(Y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop for creating actual dataset for x_train\n",
    "\n",
    "### Note: all is same as i did for train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(data_for_test))\n",
    "print(type(My_dictionary))\n",
    "X_test= np.zeros((len(data_for_test),len(My_dictionary)))\n",
    "\n",
    "for row in range(len(data_for_test)):\n",
    "    for word in data_for_test[row]:\n",
    "        for column in range(len(My_dictionary)):\n",
    "            if(word==My_dictionary[column][0]):\n",
    "                X_test[row][column]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16315, 5000)\n",
      "(3682, 5000)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From here i am applying Naive_Bayes classsifier ( Inbuilt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB(alpha=1)\n",
    "clf.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Implementing \n",
    "### 1.  Confusion matrix\n",
    "### 2.  Classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix is:\n",
      "\n",
      "[[129   1   0   0   0   0   0   0   1   0   1   2   0   1   0  11   1   0\n",
      "    2  19]\n",
      " [  1 118  14  12   2  11   4   4   0   0   0   0   3   0   1   0   0   1\n",
      "    0   0]\n",
      " [  1   7 133  19   8  16   4   1   0   0   1   1   1   0   0   0   0   0\n",
      "    0   0]\n",
      " [  1   4  15 129  19   3   3   4   0   0   1   0   9   0   0   0   0   1\n",
      "    1   0]\n",
      " [  0   1   9  25 122   0  10   1   1   0   1   0   6   0   0   0   0   0\n",
      "    0   0]\n",
      " [  1  17  11   6   0 132   1   2   0   1   0   1   1   2   0   0   0   0\n",
      "    0   0]\n",
      " [  0   1   2   7   9   0 134  11   2   0   2   1   5   0   1   0   1   1\n",
      "    0   0]\n",
      " [  0   0   0   1   2   1   5 125  15   0   1   4   6   3   0   1   7   0\n",
      "    2   1]\n",
      " [  1   0   0   0   3   0   6  19 142   0   0   0   2   1   1   0   4   0\n",
      "    1   2]\n",
      " [  0   0   1   0   1   1   4   1   5 171   6   1   1   1   1   0   2   1\n",
      "    1   0]\n",
      " [  0   1   1   0   1   0   1   0   3  10 177   0   1   3   0   1   0   0\n",
      "    1   0]\n",
      " [  1   3   3   0   1   3   0   0   1   0   0 149   1   0   0   0   4   0\n",
      "    4   1]\n",
      " [  0  11   6  15  16   0   5   4   6   1   0   1 140   1   0   0   1   0\n",
      "    0   0]\n",
      " [ 10   3   1   1   0   2   0   1   2   2   0   0   5 140   2   0   1   0\n",
      "    2   3]\n",
      " [  3   6   5   0   2   3   0   3   7   2   1   2   2   1 155   1   1   0\n",
      "    2   2]\n",
      " [ 22   0   1   0   0   0   2   0   3   1   0   1   1   5   0 150   1   2\n",
      "    1  10]\n",
      " [  0   0   0   0   0   0   0   0   1   1   0   3   1   2   0   0 139   0\n",
      "   17   6]\n",
      " [  3   0   0   0   0   0   1   1   2   0   1   0   1   2   0   3   0 142\n",
      "   12   7]\n",
      " [  4   0   1   0   0   0   0   2   0   0   1   2   2   3   3   2  12  12\n",
      "  108  22]\n",
      " [ 59   0   0   0   0   0   0   1   3   1   0   2   1   1   1  21  17   1\n",
      "   10  91]]\n",
      "\n",
      "Classification report is:\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.55      0.77      0.64       168\n",
      "          1       0.68      0.69      0.69       171\n",
      "          2       0.66      0.69      0.67       192\n",
      "          3       0.60      0.68      0.64       190\n",
      "          4       0.66      0.69      0.67       176\n",
      "          5       0.77      0.75      0.76       175\n",
      "          6       0.74      0.76      0.75       177\n",
      "          7       0.69      0.72      0.71       174\n",
      "          8       0.73      0.78      0.76       182\n",
      "          9       0.90      0.86      0.88       198\n",
      "         10       0.92      0.89      0.90       200\n",
      "         11       0.88      0.87      0.87       171\n",
      "         12       0.74      0.68      0.71       207\n",
      "         13       0.84      0.80      0.82       175\n",
      "         14       0.94      0.78      0.85       198\n",
      "         15       0.79      0.75      0.77       200\n",
      "         16       0.73      0.82      0.77       170\n",
      "         17       0.88      0.81      0.85       175\n",
      "         18       0.66      0.62      0.64       174\n",
      "         19       0.55      0.44      0.49       209\n",
      "\n",
      "avg / total       0.75      0.74      0.74      3682\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7403585008147746"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred=clf.predict(X_test)\n",
    "\n",
    "print(\"Confusion Matrix is:\")\n",
    "print()\n",
    "print(confusion_matrix(Y_test,y_test_pred))\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Classification report is:\")\n",
    "print()\n",
    "print(classification_report(Y_test,y_test_pred))\n",
    "\n",
    "clf.score(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From here i am applying Naive_bayes classifier( Own )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Own_MultinomialNB(object):\n",
    "    def __init__(self, alpha=1):\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        count_sample = X.shape[0]\n",
    "        separated = [[x for x, t in zip(X, y) if t == c] for c in np.unique(y)]\n",
    "        self.class_log_prior_ = [np.log(len(i) / count_sample) for i in separated]\n",
    "        count = np.array([np.array(i).sum(axis=0) for i in separated]) + self.alpha\n",
    "        self.feature_log_prob_ = np.log(count / count.sum(axis=1)[np.newaxis].T)\n",
    "        return self\n",
    "\n",
    "    def predict_log(self, X):\n",
    "        return [(self.feature_log_prob_ * x).sum(axis=1) + self.class_log_prior_\n",
    "                for x in X]\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.argmax(self.predict_log(X), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Own_MultinomialNB at 0x1446204b470>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_own = Own_MultinomialNB()\n",
    "clf_own.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix is:\n",
      "\n",
      "[[129   1   0   0   0   0   0   0   1   0   1   2   0   1   0  11   1   0\n",
      "    2  19]\n",
      " [  1 118  14  12   2  11   4   4   0   0   0   0   3   0   1   0   0   1\n",
      "    0   0]\n",
      " [  1   7 133  19   8  16   4   1   0   0   1   1   1   0   0   0   0   0\n",
      "    0   0]\n",
      " [  1   4  15 129  19   3   3   4   0   0   1   0   9   0   0   0   0   1\n",
      "    1   0]\n",
      " [  0   1   9  25 122   0  10   1   1   0   1   0   6   0   0   0   0   0\n",
      "    0   0]\n",
      " [  1  17  11   6   0 132   1   2   0   1   0   1   1   2   0   0   0   0\n",
      "    0   0]\n",
      " [  0   1   2   7   9   0 134  11   2   0   2   1   5   0   1   0   1   1\n",
      "    0   0]\n",
      " [  0   0   0   1   2   1   5 125  15   0   1   4   6   3   0   1   7   0\n",
      "    2   1]\n",
      " [  1   0   0   0   3   0   6  19 142   0   0   0   2   1   1   0   4   0\n",
      "    1   2]\n",
      " [  0   0   1   0   1   1   4   1   5 171   6   1   1   1   1   0   2   1\n",
      "    1   0]\n",
      " [  0   1   1   0   1   0   1   0   3  10 177   0   1   3   0   1   0   0\n",
      "    1   0]\n",
      " [  1   3   3   0   1   3   0   0   1   0   0 149   1   0   0   0   4   0\n",
      "    4   1]\n",
      " [  0  11   6  15  16   0   5   4   6   1   0   1 140   1   0   0   1   0\n",
      "    0   0]\n",
      " [ 10   3   1   1   0   2   0   1   2   2   0   0   5 140   2   0   1   0\n",
      "    2   3]\n",
      " [  3   6   5   0   2   3   0   3   7   2   1   2   2   1 155   1   1   0\n",
      "    2   2]\n",
      " [ 22   0   1   0   0   0   2   0   3   1   0   1   1   5   0 150   1   2\n",
      "    1  10]\n",
      " [  0   0   0   0   0   0   0   0   1   1   0   3   1   2   0   0 139   0\n",
      "   17   6]\n",
      " [  3   0   0   0   0   0   1   1   2   0   1   0   1   2   0   3   0 142\n",
      "   12   7]\n",
      " [  4   0   1   0   0   0   0   2   0   0   1   2   2   3   3   2  12  12\n",
      "  108  22]\n",
      " [ 59   0   0   0   0   0   0   1   3   1   0   2   1   1   1  21  17   1\n",
      "   10  91]]\n",
      "\n",
      "Classification report is:\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.55      0.77      0.64       168\n",
      "          1       0.68      0.69      0.69       171\n",
      "          2       0.66      0.69      0.67       192\n",
      "          3       0.60      0.68      0.64       190\n",
      "          4       0.66      0.69      0.67       176\n",
      "          5       0.77      0.75      0.76       175\n",
      "          6       0.74      0.76      0.75       177\n",
      "          7       0.69      0.72      0.71       174\n",
      "          8       0.73      0.78      0.76       182\n",
      "          9       0.90      0.86      0.88       198\n",
      "         10       0.92      0.89      0.90       200\n",
      "         11       0.88      0.87      0.87       171\n",
      "         12       0.74      0.68      0.71       207\n",
      "         13       0.84      0.80      0.82       175\n",
      "         14       0.94      0.78      0.85       198\n",
      "         15       0.79      0.75      0.77       200\n",
      "         16       0.73      0.82      0.77       170\n",
      "         17       0.88      0.81      0.85       175\n",
      "         18       0.66      0.62      0.64       174\n",
      "         19       0.55      0.44      0.49       209\n",
      "\n",
      "avg / total       0.75      0.74      0.74      3682\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_test_pred1=clf_own.predict(X_test)\n",
    "\n",
    "print(\"Confusion Matrix is:\")\n",
    "print()\n",
    "print(confusion_matrix(Y_test,y_test_pred1))\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Classification report is:\")\n",
    "print()\n",
    "print(classification_report(Y_test,y_test_pred1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7403585008147746"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test,Y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8397793441618143"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.65746753 0.69543974 0.6685761 ]\n",
      "time is : 29.508705174922945 mins\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "print(cross_val_score(clf,X_test,Y_test))\n",
    "print('time is :',(time.time()-start)/60, 'mins')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note : increasing number of features it gives better score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
